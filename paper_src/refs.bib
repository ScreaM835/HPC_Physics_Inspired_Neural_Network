@article{mcculloch1943logical,
  title={A Logical Calculus of Ideas Immanent in Nervous Activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={Bulletin of Mathematical Biophysics},
  volume={5},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group},
  doi={10.1038/323533a0},
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{Goodfellow-et-al-2016,
  title={Deep Learning},
  author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher={MIT Press},
  note={\url{http://www.deeplearningbook.org}},
  year={2016}
}

@Article{rosenblatt1958perceptron,
  title = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  author = {Rosenblatt, Frank},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  year = {1958},
  doi = {10.1037/h0042519},
  url = {https://doi.org/10.1037/h0042519},
}

@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


@misc{haddou2023quasinormal,
      title={Quasi-normal modes of near-extremal black holes in dRGT massive gravity using Physics-Informed Neural Networks (PINNs)}, 
      author={Marwan Ait Haddou},
      year={2023},
      eprint={2303.02395},
      archivePrefix={arXiv},
      primaryClass={gr-qc}
}

@misc{font,
      title={Gradient-Annihilated PINNs for Solving Riemann Problems: Application to Relativistic Hydrodynamics}, 
      author={Antonio Ferrer-Sánchez and José D. Martín-Guerrero and Roberto Ruiz de Austri and Alejandro Torres-Forné and José A. Font},
      year={2023},
      eprint={2305.08448},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph}
}

@article{RAISSI2019686,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{Blechschmidt_Ernst_2021, 
title={Three ways to solve partial differential equations with neural networks — a review}, 
volume={44}, 
url={https://onlinelibrary.wiley.com/doi/full/10.1002/gamm.202100006}, 
DOI={10.1002/gamm.202100006}, 
number={2}, 
journal={GAMM-Mitteilungen}, 
author={Blechschmidt, Jan and Ernst, Oliver G.}, 
year={2021}, 
month={May}
}

@ARTICLE{Kokkotas1999-rw,
  title    = "{Quasi-Normal} Modes of Stars and Black Holes",
  author   = "Kokkotas, Kostas D and Schmidt, Bernd G",
  abstract = "Perturbations of stars and black holes have been one of the main
              topics of relativistic astrophysics for the last few decades.
              They are of particular importance today, because of their
              relevance to gravitational wave astronomy. In this review we
              present the theory of quasi-normal modes of compact objects from
              both the mathematical and astrophysical points of view. The
              discussion includes perturbations of black holes (Schwarzschild,
              Reissner-Nordstr{\"o}m, Kerr and Kerr-Newman) and relativistic
              stars (non-rotating and slowly-rotating). The properties of the
              various families of quasi-normal modes are described, and
              numerical techniques for calculating quasi-normal modes reviewed.
              The successes, as well as the limits, of perturbation theory are
              presented, and its role in the emerging era of numerical
              relativity and supercomputers is discussed.",
  journal  = "Living Reviews in Relativity",
  volume   =  2,
  number   =  1,
  pages    = "2",
  month    =  sep,
  year     =  1999
}

@article{lu2021deepxde,
  author  = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  title   = {{DeepXDE}: A deep learning library for solving differential equations},
  journal = {SIAM Review},
  volume  = {63},
  number  = {1},
  pages   = {208-228},
  year    = {2021},
  doi     = {10.1137/19M1274067}
}

@article{Chandrasekhar_1975,
  author  = {Chandrasekhar Subrahmanyan and Detweiler S.},
  title   = {The quasi-normal modes of the Schwarzschild black hole},
  journal = {Proc. R. Soc. Lond.},
  volume  = {344},
  number  = {1639},
  pages   = {441–452},
  year    = {1975},
  doi     = {10.1098/rspa.1975.0112}
}

@article{YU2022114823,
title = {Gradient-enhanced physics-informed neural networks for forward and inverse PDE problems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114823},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114823},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522001438},
author = {Jeremy Yu and Lu Lu and Xuhui Meng and George Em Karniadakis},
keywords = {Deep learning, Partial differential equations, Physics-informed neural networks, Gradient-enhanced, Residual-based adaptive refinement},
abstract = {Deep learning has been shown to be an effective tool in solving partial differential equations (PDEs) through physics-informed neural networks (PINNs). PINNs embed the PDE residual into the loss function of the neural network, and have been successfully employed to solve diverse forward and inverse PDE problems. However, one disadvantage of the first generation of PINNs is that they usually have limited accuracy even with many training points. Here, we propose a new method, gradient-enhanced physics-informed neural networks (gPINNs), for improving the accuracy of PINNs. gPINNs leverage gradient information of the PDE residual and embed the gradient into the loss function. We tested gPINNs extensively and demonstrated the effectiveness of gPINNs in both forward and inverse PDE problems. Our numerical results show that gPINN performs better than PINN with fewer training points. Furthermore, we combined gPINN with the method of residual-based adaptive refinement (RAR), a method for improving the distribution of training points adaptively during training, to further improve the performance of gPINN, especially in PDEs with solutions that have steep gradients.}
}

@article{WU2023115671,
title = {A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {403},
pages = {115671},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.115671},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522006260},
author = {Chenxi Wu and Min Zhu and Qinyang Tan and Yadhu Kartha and Lu Lu},
keywords = {Partial differential equations, Physics-informed neural networks, Residual point distribution, Non-adaptive uniform sampling, Uniform sampling with resampling, Residual-based adaptive sampling},
abstract = {Physics-informed neural networks (PINNs) have shown to be effective tools for solving both forward and inverse problems of partial differential equations (PDEs). PINNs embed the PDEs into the loss of the neural network using automatic differentiation, and this PDE loss is evaluated at a set of scattered spatio-temporal points (called residual points). The location and distribution of these residual points are highly important to the performance of PINNs. However, in the existing studies on PINNs, only a few simple residual point sampling methods have mainly been used. Here, we present a comprehensive study of two categories of sampling for PINNs: non-adaptive uniform sampling and adaptive nonuniform sampling. We consider six uniform sampling methods, including (1) equispaced uniform grid, (2) uniformly random sampling, (3) Latin hypercube sampling, (4) Halton sequence, (5) Hammersley sequence, and (6) Sobol sequence. We also consider a resampling strategy for uniform sampling. To improve the sampling efficiency and the accuracy of PINNs, we propose two new residual-based adaptive sampling methods: residual-based adaptive distribution (RAD) and residual-based adaptive refinement with distribution (RAR-D), which dynamically improve the distribution of residual points based on the PDE residuals during training. Hence, we have considered a total of 10 different sampling methods, including six non-adaptive uniform sampling, uniform sampling with resampling, two proposed adaptive sampling, and an existing adaptive sampling. We extensively tested the performance of these sampling methods for four forward problems and two inverse problems in many setups. Our numerical results presented in this study are summarized from more than 6000 simulations of PINNs. We show that the proposed adaptive sampling methods of RAD and RAR-D significantly improve the accuracy of PINNs with fewer residual points for both forward and inverse problems. The results obtained in this study can also be used as a practical guideline in choosing sampling methods.}
}

@article{Wang2021,
author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
title = {Understanding and Mitigating Gradient Flow Pathologies in Physics-Informed Neural Networks},
journal = {SIAM Journal on Scientific Computing},
volume = {43},
number = {5},
pages = {A3055-A3081},
year = {2021},
doi = {10.1137/20M1318043},
URL = {https://doi.org/10.1137/20M1318043},
eprint = { https://doi.org/10.1137/20M1318043},
abstract = { The widespread use of neural networks across different scientific domains often involves constraining them to satisfy certain symmetries, conservation laws, or other domain knowledge. Such constraints are often imposed as soft penalties during model training and effectively act as domain-specific regularizers of the empirical risk loss. Physics-informed neural networks is an example of this philosophy in which the outputs of deep neural networks are constrained to approximately satisfy a given set of partial differential equations. In this work we review recent advances in scientific machine learning with a specific focus on the effectiveness of physics-informed neural networks in predicting outcomes of physical systems and discovering hidden physics from noisy data. We also identify and analyze a fundamental mode of failure of such approaches that is related to numerical stiffness leading to unbalanced back-propagated gradients during model training. To address this limitation we present a learning rate annealing algorithm that utilizes gradient statistics during model training to balance the interplay between different terms in composite loss functions. We also propose a novel neural network architecture that is more resilient to such gradient pathologies. Taken together, our developments provide new insights into the training of constrained neural networks and consistently improve the predictive accuracy of physics-informed neural networks by a factor of 50--100\$\times\$ across a range of problems in computational physics. All code and data accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs. }
}

@article{Wang2022Causality, 
title={Respecting causality is all you need for training physics-informed Neural Networks}, 
url={https://arxiv.org/abs/2203.07404}, 
journal={arXiv.org}, 
author={Wang, Sifan and Sankaran, Shyam and Perdikaris, Paris}, 
year={2022}, 
month={Mar}
} 

@article{WANG2022110768,
title = {When and why PINNs fail to train: A neural tangent kernel perspective},
journal = {Journal of Computational Physics},
volume = {449},
pages = {110768},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110768},
url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
author = {Sifan Wang and Xinling Yu and Paris Perdikaris},
keywords = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.}
}

@article{WANG2021113938,
title = {On the eigenvector bias of Fourier feature networks: From regression to solving multi-scale PDEs with physics-informed neural networks},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {384},
pages = {113938},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113938},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521002759},
author = {Sifan Wang and Hanwen Wang and Paris Perdikaris},
keywords = {Spectral bias, Deep learning, Neural Tangent Kernel, Partial differential equations, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) are demonstrating remarkable promise in integrating physical models with gappy and noisy observational data, but they still struggle in cases where the target functions to be approximated exhibit high-frequency or multi-scale features. In this work we investigate this limitation through the lens of Neural Tangent Kernel (NTK) theory and elucidate how PINNs are biased towards learning functions along the dominant eigen-directions of their limiting NTK. Using this observation, we construct novel architectures that employ spatio-temporal and multi-scale random Fourier features, and justify how such coordinate embedding layers can lead to robust and accurate PINN models. Numerical examples are presented for several challenging cases where conventional PINN models fail, including wave propagation and reaction–diffusion dynamics, illustrating how the proposed methods can be used to effectively tackle both forward and inverse problems involving partial differential equations with multi-scale behavior. All code an data accompanying this manuscript will be made publicly available at https://github.com/PredictiveIntelligenceLab/MultiscalePINNs.}
}

@article{Bellman,
author = {Richard Bellman },
title = {Dynamic Programming},
journal = {Science},
volume = {153},
number = {3731},
pages = {34-37},
year = {1966},
doi = {10.1126/science.153.3731.34},
URL = {https://www.science.org/doi/abs/10.1126/science.153.3731.34},
eprint = {https://www.science.org/doi/pdf/10.1126/science.153.3731.34},
abstract = {Little has been done in the study of these intriguing questions, and I do not wish to give the impression that any extensive set of ideas exists that could be called a "theory." What is quite surprising, as far as the histories of science and philosophy are concerned, is that the major impetus for the fantastic growth of interest in brain processes, both psychological and physiological, has come from a device, a machine, the digital computer. In dealing with a human being and a human society, we enjoy the luxury of being irrational, illogical, inconsistent, and incomplete, and yet of coping. In operating a computer, we must meet the rigorous requirements for detailed instructions and absolute precision. If we understood the ability of the human mind to make effective decisions when confronted by complexity, uncertainty, and irrationality, then we could use computers a million times more effectively than we do. Recognition of this fact has been a motivation for the spurt of research in the field of neurophysiology. The more we study the information-processing aspects of the mind, the more perplexed and impressed we become. It will be a very long time before we understand these processes sufficiently to reproduce them. In any case, the mathematician sees hundreds and thousands of formidable new problems in dozens of blossoming areas, puzzles galore, and challenges to his heart's content. He may never resolve some of these, but he will never be bored. What more can he ask?}}

@article{article,
author = {Powell, Warren},
year = {2009},
month = {04},
pages = {239-249},
title = {What You Should Know About Approximate Dynamic Programming},
volume = {56},
journal = {Naval Research Logistics - NAV RES LOG},
doi = {10.1002/nav.20347}
}

@article{baydin,
author = {Baydin, At\i{}l\i{}m G\"{u}nes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
title = {Automatic Differentiation in Machine Learning: A Survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {5595–5637},
numpages = {43},
keywords = {differentiable programming, backpropagation}
}

@article{Freire2022,
author = {Freire, Pedro and Srivallapanondh, Sasipim and Napoli, Antonio and Prilepsky, Jaroslaw and Turitsyn, Sergei},
year = {2022},
month = {06},
pages = {},
title = {Computational Complexity Evaluation of Neural Network Applications in Signal Processing},
doi = {10.48550/arXiv.2206.12191}
}

@misc{freire2022computational,
      title={Computational Complexity Evaluation of Neural Network Applications in Signal Processing}, 
      author={Pedro J. Freire and Sasipim Srivallapanondh and Antonio Napoli and Jaroslaw E. Prilepsky and Sergei K. Turitsyn},
      year={2022},
      eprint={2206.12191},
      archivePrefix={arXiv},
      primaryClass={eess.SP}
}
